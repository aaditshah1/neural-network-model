# Neural Network Models from Scratch

A comprehensive implementation of four different neural network models built from scratch using NumPy, demonstrating fundamental machine learning concepts from basic perceptrons to recurrent neural networks.

## ðŸ§  Models Implemented

### 1. Perceptron Model
- Binary classification using single-layer perceptron
- Learns linear decision boundaries
- Converges to 100% accuracy on linearly separable data

### 2. Regression Model
- Approximates sin(x) function on interval [-2Ï€, 2Ï€]
- Architecture: 1 â†’ 512 (ReLU) â†’ 1
- Achieves loss < 0.02 using gradient descent

### 3. Digit Classification (MNIST)
- Handwritten digit recognition (0-9)
- Architecture: 784 â†’ 200 (ReLU) â†’ 10
- Achieves 97%+ test accuracy on MNIST dataset

### 4. Language Identification (RNN)
- Identifies language from words using Recurrent Neural Network
- Languages: English, Spanish, Finnish, Dutch, Polish
- Architecture: RNN with 200 hidden units + 2 dense layers
- Achieves 82%+ test accuracy

## ðŸ“ Project Structure

```
â”œâ”€â”€ nn.py                    # Neural network framework (nodes, backprop)
â”œâ”€â”€ models.py                # Four model implementations
â”œâ”€â”€ backend.py               # Dataset handling and visualization
â”œâ”€â”€ autograder.py           # Testing framework
â”œâ”€â”€ analysis_and_visualization.py  # Comprehensive analysis script
â”œâ”€â”€ requirements.txt        # Python dependencies
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ mnist.npz          # MNIST dataset
â”‚   â””â”€â”€ lang_id.npz        # Language identification dataset
â””â”€â”€ evidence_outputs/      # Generated visualizations
```

## ðŸš€ Quick Start

### Installation

```bash
# Clone the repository
git clone <your-repo-url>
cd <repo-name>

# Install dependencies
pip install -r requirements.txt
```

### Running the Models

```bash
# Run all models with visualization
python backend.py

# Run autograder tests
python autograder.py

# Test specific model
python autograder.py -q q1  # Perceptron
python autograder.py -q q2  # Regression
python autograder.py -q q3  # MNIST
python autograder.py -q q4  # Language ID
```

## ðŸ“Š Results

| Model | Metric | Result |
|-------|--------|--------|
| Perceptron | Accuracy | 100% |
| Regression | Loss | < 0.02 |
| MNIST Digits | Test Accuracy | 97-98% |
| Language ID | Test Accuracy | 82-85% |

## ðŸ› ï¸ Technical Details

### Custom Neural Network Framework

The project includes a custom neural network framework (`nn.py`) implementing:

- **Node Types**: Parameter, Constant, FunctionNode
- **Operations**: Linear, Add, AddBias, ReLU, DotProduct
- **Loss Functions**: SquareLoss (MSE), SoftmaxLoss (Cross-Entropy)
- **Optimization**: Automatic differentiation via backpropagation

### Key Concepts Demonstrated

- âœ… Forward/backward propagation
- âœ… Gradient descent optimization
- âœ… Multi-layer perceptrons
- âœ… Activation functions (ReLU)
- âœ… Recurrent neural networks
- âœ… Classification and regression
- âœ… Model evaluation and visualization

## ðŸ“ˆ Sample Outputs

### MNIST Digit Grid
![MNIST Grid](evidence_outputs/mnist_digit_grid.png)

### Training Progress
![Accuracy Curve](evidence_outputs/mnist_accuracy_curve.png)

## ðŸ“‹ Requirements

- Python 3.7+
- NumPy
- Matplotlib
- Seaborn

## ðŸŽ“ Learning Outcomes

This project demonstrates understanding of:
- Neural network architecture design
- Backpropagation and gradient computation
- Training loop implementation
- Model evaluation metrics
- Sequence modeling with RNNs

## ðŸ“ License

This is an educational project. Feel free to use for learning purposes.

## ðŸ¤ Acknowledgments

- MNIST dataset for handwritten digit classification
- Custom neural network framework built from scratch
- Implements concepts from machine learning fundamentals

